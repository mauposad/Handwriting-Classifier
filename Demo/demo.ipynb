{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual demonstration of trained CNN\n",
    "\n",
    "### Attempt at Graphic user interface demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import tkinter as tk\n",
    "from PIL import Image\n",
    "from PIL import ImageTk\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Model architecture followed by initializing model w/ saved parameters from last test session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "        def __init__(self,printtoggle):\n",
    "            super().__init__()\n",
    "\n",
    "\n",
    "            #print toggle\n",
    "            self.print = printtoggle\n",
    "\n",
    "            #first convolutional layer\n",
    "            # in channel, out channel, kernel size, padding\n",
    "            self.conv1 = nn.Conv2d(1,6,kernel_size=3,padding=1)\n",
    "            # batch normalization\n",
    "            self.bnorm1 = nn.BatchNorm2d(6) # input the number of channels on this layer\n",
    "\n",
    "            #second convolutional layer\n",
    "            self.conv2 = nn.Conv2d(6,6,kernel_size=3,padding=1,stride=1)\n",
    "            self.bnorm2 = nn.BatchNorm2d(6)\n",
    "\n",
    "\n",
    "            #linear decision layers\n",
    "            \"\"\"compute the output size for the convolutional layers\n",
    "                then make it the input size for the linear layers\"\"\"\n",
    "            self.fc1 = nn.Linear(7*7*6,50) \n",
    "            self.fc2 = nn.Linear(50,27)\n",
    "    \n",
    "        def forward(self, x):\n",
    "            if self.print: print(f'Input: {list(x.shape)}')\n",
    "\n",
    "            # first block: convolution -> maxpool -> batchnorm -> relu\n",
    "            x = F.max_pool2d(self.conv1(x),2)\n",
    "            x = F.relu(self.bnorm1(x))\n",
    "            if self.print: print(f'First CPR block: {list(x.shape)}')\n",
    "\n",
    "            # second block \n",
    "            x = F.max_pool2d(self.conv2(x),2)\n",
    "            x = F.relu(self.bnorm2(x))\n",
    "            if self.print: print(f'Second CPR block: {list(x.shape)}')\n",
    "\n",
    "            # reshape for linear layer\n",
    "            nUnits = x.shape.numel()/x.shape[0]\n",
    "            x = x.view(-1,int(nUnits))\n",
    "            if self.print: print(f'Vectorized: {list(x.shape)}')\n",
    "\n",
    "            #linear layers\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.fc2(x)\n",
    "            if self.print: print(f'Final output: {list(x.shape)}')\n",
    "\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bnorm1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bnorm2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=294, out_features=50, bias=True)\n",
       "  (fc2): Linear(in_features=50, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN(False)\n",
    "model.load_state_dict(torch.load('CNN.pth'))\n",
    "model.eval() # setting it to evaluation mode( parameters behave differently than training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with GUI, \n",
    "    Makes popup window for user to draw on\n",
    "    saves the image from popup window and inverts colors \n",
    "    I am inverting the color as the model was trained on the inverted color scheme of the pop up window\n",
    "    then data pipeline is created so model can properly classifiy image/ letter\n",
    "    (one difference is images were not normalized and model performance increased in this step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet_mapping = {\n",
    "    1: 'A', 2: 'B', 3: 'C', 4: 'D', 5: 'E', 6: 'F', 7: 'G', 8: 'H', 9: 'I', 10: 'J',\n",
    "    11: 'K', 12: 'L', 13: 'M', 14: 'N', 15: 'O', 16: 'P', 17: 'Q', 18: 'R', 19: 'S',\n",
    "    20: 'T', 21: 'U', 22: 'V', 23: 'W', 24: 'X', 25: 'Y', 26: 'Z'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 13\n",
      "Predicted class: M\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 2\n",
      "Predicted class: B\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAILUlEQVR4nO3cv6vX1QPH8fe9Gd7uFEbaBefoPyiFEJfAQUFwbXCRhgandiHwH0hcW/wB4eAi4hZtIuh4HVwUChGC7o1K5Ppp8fvkCzl4Png/Xq+Px3xfvA+lPe8ZOkuz2Ww2AcA0Tctv+gAA7ByiAEBEAYCIAgARBQAiCgBEFACIKACQPa/6g0tLS9t5DgC22av8v8puCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBkz5s+ALzNPv300+HNb7/9Nte3Njc359rBCDcFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgXkmFF86ePTu8OX/+/PDm7t27w5tpmqZjx44Nb/7444+5vsW7y00BgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgDEg3jwwpdffjm8WVlZGd4cOnRoeDNN03Tw4MHhjQfxGOWmAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUA4kE8eGFjY2Mh33n27Nlcu62trdd8EvgvNwUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABAP4sELi3pwbnl5vt/FVlZWXvNJ4L/cFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQDyIBy88fPhwId9577335tqtra0Nb+7duzfXt3h3uSkAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgDxSiq88OzZszd9BHjj3BQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgDZ86YPwOt1+PDh4c3nn38+vNmzZ3F/dJ48eTK8uXLlyvBma2treAO7jZsCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQr6TuUGfOnJlr98MPPwxv3n///bm+tZOdPHlyeHP//v1tOAm8XdwUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBAPIi3AAcPHhzefP/993N9azc+bjePEydODG/+/vvvbTgJvF3cFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQDyItwCnT58e3nz88cfbcJKXu3r16vDmwoULw5uPPvpoeDNN03Tx4sXhzdra2vDmgw8+GN7AbuOmAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUA4kG8Qaurq8Obr7/+ehtO8nJ//fXX8ObcuXPDm/X19eHNvLa2toY3169fH94sL/sdCfwtACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIA8SDeoH379g1vPvnkk204ycvdvn17eLPIx+3mcePGjeHNL7/8Mrw5cuTI8AZ2GzcFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgXkkdNJvNhjfPnz/fhpO83J9//rmwby3KPP/8Ll26NLzxSiq4KQDwf0QBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQDiQTx2pZs3bw5vHj9+PLw5cODA8AZ2MjcFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQD+INev78+fBmNpttw0lebnV1dWHf2skePXo0vLl+/frw5syZM8Mb2MncFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQDyIN+j3338f3jx+/Hh48+GHHw5vpmmavvjii+HNV199Nby5devW8Ganm+ff7SItL/sdju3nTxkAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgH8QY9ffp0eHP58uXhzblz54Y30zRNq6urw5tr164Nb3788cfhza+//jq8WaSjR48u5DsbGxtz7R48ePCaTwL/5aYAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBkaTabzV7pB5eWtvssu9bevXuHNz/99NNc3zp+/PhcOxZnnldpp2maTp069ZpPwrvmVf5z76YAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQDiQbwdap5H9KZpmr777rvhzbfffju82b9///BmN1pfXx/efPPNN3N96+eff55rB//jQTwAhogCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgDEg3hMa2trw5vPPvtseLOysjK8WaR//vlneHPnzp3hzebm5vAGXgcP4gEwRBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACAexAN4R3gQD4AhogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAMieV/3B2Wy2necAYAdwUwAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIP8Crn7cfp6OGzoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 4\n",
      "Predicted class: D\n"
     ]
    }
   ],
   "source": [
    "def classify_image(model, dataloader):\n",
    "     # Iterate over the dataloader to get batches of data\n",
    "    for batch in dataloader:\n",
    "        # Extract the input tensor from the batch\n",
    "        input_tensor = batch[0]  # Assuming the input tensor is at index 0\n",
    "        \n",
    "        # Perform inference with the model\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor) # Forward pass\n",
    "        \n",
    "        probabilities = F.softmax(output, dim=1)  # Perform post-processing (e.g., softmax) on the output\n",
    "    \n",
    "        predicted_class = torch.argmax(probabilities, dim=1)  # Get the predicted class index\n",
    "        print(\"Predicted class:\", predicted_class.item())\n",
    "        \n",
    "        # Optionally, you can return the predicted class or probabilities for further processing\n",
    "        return predicted_class.item(), probabilities.numpy()\n",
    "\n",
    "def display_image_from_dataloader(dataloader):\n",
    "    # Iterate over the dataloader to get a single batch\n",
    "    for batch in dataloader:\n",
    "        # Extract the input tensor from the batch\n",
    "        input_tensor = batch[0]  # Assuming the input tensor is at index 0\n",
    "        \n",
    "        # Convert the tensor to a NumPy array and reshape it\n",
    "        img_array = input_tensor.numpy().squeeze()\n",
    "        \n",
    "        # Display the image using Matplotlib\n",
    "        plt.imshow(img_array, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def clear_canvas():\n",
    "    canvas.delete(\"all\")\n",
    "\n",
    "def draw(event):\n",
    "    x, y = event.x, event.y\n",
    "    r = 24  # Adjust the size of the brush\n",
    "    canvas.create_oval(x - r, y - r, x + r, y + r, fill='black')\n",
    "\n",
    "def save_drawing():\n",
    "    \n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    filename = f\"drawn_image_{timestamp}.eps\"\n",
    "    canvas.postscript(file=filename, colormode='color')  # Save the canvas contents as a PostScript file\n",
    "    img = Image.open(filename) # Open the saved PostScript file using PIL\n",
    "\n",
    "    img_grayscale = img.convert('L') # for correct number of channels\n",
    "    img_resized = img_grayscale.resize((28, 28))\n",
    "    img_array = np.array(img_resized) # Convert the PIL image to a NumPy array\n",
    "    img_inverted = np.invert(img_array) # invert from white to black\n",
    "    img_reshaped = img_inverted.reshape((28,28)) # 28 by 28 matrix\n",
    "    \n",
    "    img_tensor = torch.from_numpy(img_reshaped)\n",
    "    img_tensor_float = img_tensor.to(torch.float32)\n",
    "\n",
    "\n",
    "    # img_tensor_normalized = F.normalize(img_tensor_float, dim = 1) # normalizing tensor values\n",
    "    img_tensor_batched = img_tensor_float.unsqueeze(0).unsqueeze(0) # Adding correct batch value & channel dimensions\n",
    "    \n",
    "    img_array = img_tensor_batched.squeeze().numpy()  # Squeeze to remove batch and channel dimensions\n",
    "\n",
    "    # Plot the image\n",
    "    plt.imshow(img_array, cmap='gray')  # Assuming grayscale image\n",
    "    plt.axis('off')  # Turn off axis\n",
    "    plt.show()\n",
    "\n",
    "    dataset = TensorDataset(img_tensor_batched)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    # display_image_from_dataloader(dataloader)\n",
    "\n",
    "    predicted_class, probabilities = classify_image(model, dataloader)\n",
    "    predicted_letter = alphabet_mapping[predicted_class]        \n",
    "    print(\"Predicted class:\", predicted_letter)\n",
    "\n",
    "\n",
    "# Create the main window\n",
    "root = tk.Tk()\n",
    "root.title(\"Handwriting Classification Demo\")\n",
    "\n",
    "# Create a canvas where users can draw\n",
    "canvas = tk.Canvas(root, width=500, height=500, bg='white', highlightthickness=0)\n",
    "canvas.pack()\n",
    "\n",
    "# Bind mouse events to the canvas\n",
    "canvas.bind(\"<B1-Motion>\", draw)\n",
    "\n",
    "# Create buttons\n",
    "clear_button = tk.Button(root, text=\"Clear\", command=clear_canvas)\n",
    "clear_button.pack(side=tk.LEFT)\n",
    "save_button = tk.Button(root, text=\"Save Drawing\", command=save_drawing)\n",
    "save_button.pack(side=tk.LEFT)\n",
    "\n",
    "# Run the GUI loop\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
